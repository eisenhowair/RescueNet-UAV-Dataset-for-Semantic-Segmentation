"""
Masks are generated by my own scritps. Mask of all classes in uint8 format means multi hot encoded. 
This script tries to segment all types of classes.
Classes are: {'Background':0, 'Water':1, 'Building_No_Damage':2, 'Building_Minor_Damage':3, 'Building_Major_Damage':4, 'Building_Total_Destruction':5, 
            'Vehicle':6, 'Road-Clear':7, 'Road-Blocked':8, 'Tree':9, 'Pool':10}
"""

import os
from collections import OrderedDict
import torch.utils.data as data
from . import utils
import cv2

# @ sh:add
from PIL import Image, ImageOps, ImageFilter
from torchvision import transforms
import numpy as np
import torch
import albumentations as A
from albumentations.pytorch import ToTensorV2


class RescueNet(data.Dataset):
    """RescueNet-v2.0 dataset: ....

    Keyword arguments:
    - root_dir (``string``): Root directory path.
    - mode (``string``): The type of dataset: 'train' for training set, 'val'
    for validation set, and 'test' for test set.
    - transform (``callable``, optional): A function/transform that  takes in
    an PIL image and returns a transformed version. Default: None.
    - label_transform (``callable``, optional): A function/transform that takes
    in the target and transforms it. Default: None.
    - loader (``callable``, optional): A function to load an image given its
    path. By default ``default_loader`` is used.

    """

    # Training dataset root folders
    train_folder = "train/train-org-img/"
    train_lbl_folder = "train/train-label-img_original/"

    # Validation dataset root folders
    val_folder = "val/val-org-img/"
    val_lbl_folder = "val/val-label-img_original/"

    # Test dataset root folders
    test_folder = "test/test-org-img/"
    test_lbl_folder = "test/test-label-img_original/"

    # Filters to find the images
    org_img_extension = ".jpg"
    # lbl_name_filter = '.png'

    lbl_img_extension = ".png"
    lbl_name_filter = "lab"

    # The values associated with the 35 classes
    full_classes = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
    # The values above are remapped to the following
    new_classes = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

    # Default encoding for pixel value, class name, and class color
    color_encoding = OrderedDict(
        [
            ("unlabeled", (0, 0, 0)),
            ("water", (61, 230, 250)),
            ("building-no-damage", (180, 120, 120)),
            ("building-medium-damage", (235, 255, 7)),
            ("building-major-damage", (255, 184, 6)),
            ("building-total-destruction", (255, 0, 0)),
            ("vehicle", (255, 0, 245)),
            ("road-clear", (140, 140, 140)),
            ("road-blocked", (160, 150, 20)),
            ("tree", (4, 250, 7)),
            ("pool", (255, 235, 0)),
        ]
    )

    def __init__(
        self,
        root_dir,
        mode="train",
        transform=None,
        label_transform=None,
        albumentations_transform=None,
        transfo_activated=False,
        loader=utils.pil_loader,
    ):
        self.root_dir = root_dir
        self.mode = mode
        self.transform = transform
        self.label_transform = label_transform
        self.loader = loader
        self.mean = [0.485, 0.456, 0.406]
        self.std = [0.229, 0.224, 0.225]
        self.normalize = transforms.Normalize(self.mean, self.std)
        self.transfo_activated = transfo_activated

        if self.mode.lower() == "train":
            # Get the training data and labels filepaths
            self.train_data = utils.get_files(
                os.path.join(root_dir, self.train_folder),
                extension_filter=self.org_img_extension,
            )

            self.train_labels = utils.get_files(
                os.path.join(root_dir, self.train_lbl_folder),
                name_filter=self.lbl_name_filter,
                extension_filter=self.lbl_img_extension,
            )
        elif self.mode.lower() == "val":
            # Get the validation data and labels filepaths
            self.val_data = utils.get_files(
                os.path.join(root_dir, self.val_folder),
                extension_filter=self.org_img_extension,
            )

            self.val_labels = utils.get_files(
                os.path.join(root_dir, self.val_lbl_folder),
                name_filter=self.lbl_name_filter,
                extension_filter=self.lbl_img_extension,
            )
        elif self.mode.lower() == "test":
            # Get the test data and labels filepaths
            self.test_data = utils.get_files(
                os.path.join(root_dir, self.test_folder),
                extension_filter=self.org_img_extension,
            )

            self.test_labels = utils.get_files(
                os.path.join(root_dir, self.test_lbl_folder),
                name_filter=self.lbl_name_filter,
                extension_filter=self.lbl_img_extension,
            )
        elif self.mode.lower() == "vis":
            # Get the test data and labels filepaths
            self.test_data = utils.get_files(
                os.path.join(root_dir, self.test_folder),
                extension_filter=self.org_img_extension,
            )

            self.test_labels = utils.get_files(
                os.path.join(root_dir, self.test_lbl_folder),
                name_filter=self.lbl_name_filter,
                extension_filter=self.lbl_img_extension,
            )
        else:
            raise RuntimeError(
                "Unexpected dataset mode. " "Supported modes are: train, val and test"
            )

        if self.mode.lower() == "train" and self.transfo_activated == True:
            self.albumentations_transform = A.Compose(
                [
                    A.HorizontalFlip(p=0.5),
                    A.VerticalFlip(p=0.5),
                    A.RandomRotate90(p=0.5),
                    A.ShiftScaleRotate(
                        shift_limit=0.0625, scale_limit=0.2, rotate_limit=10, p=0.5
                    ),
                    A.OneOf(
                        [
                            A.RandomBrightnessContrast(p=0.5),
                            A.RandomGamma(p=0.5),
                            A.CLAHE(p=0.5),
                        ],
                        p=0.5,
                    ),
                    A.Normalize(mean=self.mean, std=self.std),
                    ToTensorV2(),
                ]
            )
        else:
            self.albumentations_transform = None

    def _normalize(self, image):
        image = image.astype(np.float32)[:, :, ::-1]
        image = image / 255.0
        image -= self.mean
        image /= self.std
        return image

    def __getitem__(self, index):
        """
        Args:
        - index (``int``): index of the item in the dataset

        Returns:
        A tuple of ``PIL.Image`` (image, label) where label is the ground-truth
        of the image.

        """

        if self.mode == "vis":
            img = Image.open(self.test_data[index]).convert("RGB")
            if self.transform is not None:
                img = self.transform(img)
            return self.normalize(img), os.path.basename(self.test_data[index])

        if self.mode.lower() == "train":
            data_path, label_path = self.train_data[index], self.train_labels[index]

        elif self.mode.lower() == "val":
            data_path, label_path = self.val_data[index], self.val_labels[index]
            self.albumentations_transform = None

        elif self.mode.lower() == "test":
            self.albumentations_transform = None
            data_path, label_path = self.test_data[index], self.test_labels[index]
        else:
            raise RuntimeError(
                "Unexpected dataset mode. " "Supported modes are: train, val and test"
            )

        img, label = self.loader(data_path, label_path)

        # Remap class labels
        label = utils.remap(label, self.full_classes, self.new_classes)

        if self.transform is not None:
            img = self.transform(img)

        if self.label_transform is not None:
            label = self.label_transform(label)

        if self.albumentations_transform is not None:
            if isinstance(img, torch.Tensor):
                # Si `img` est déjà un tenseur, on le garde tel quel
                img = (
                    img.permute(2, 0, 1).float()
                    if img.ndimension() == 3
                    else img.unsqueeze(0).float()
                )
            else:
                # Si `img` n'est pas un tenseur, on le convertit
                img = (
                    torch.from_numpy(img).permute(2, 0, 1).float()
                    if img.ndimension() == 3
                    else torch.from_numpy(img).unsqueeze(0).float()
                )

            if isinstance(label, torch.Tensor):
                # Si `label` est déjà un tenseur, on le garde tel quel
                label = (
                    label.unsqueeze(0).long()
                    if label.ndimension() == 2
                    else label.long()
                )
            else:
                # Si `label` n'est pas un tenseur, on le convertit
                label = (
                    torch.from_numpy(label).unsqueeze(0).long()
                    if label.ndimension() == 2
                    else torch.from_numpy(label).long()
                )

            # Vérification des dimensions du masque
            if (
                img.shape[:2] != label.shape[:2]
            ):  # Comparaison des dimensions (hauteur, largeur)
                label = cv2.resize(
                    label, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST
                )

            augmented = self.albumentations_transform(image=img, mask=label)
            img = augmented["image"]
            label = augmented["mask"]

            # Réorganisation des canaux si nécessaire
            if len(img.shape) == 3:  # Vérifie que c'est (H, W, C)
                img = (
                    torch.from_numpy(img).permute(2, 0, 1).float()
                )  # Convertit en (C, H, W)
            else:
                img = (
                    torch.from_numpy(img).unsqueeze(0).float()
                )  # Si c'est grayscale (H, W) -> (1, H, W)

            if len(label.shape) == 2:  # Masque de dimension (H, W)
                label = (
                    torch.from_numpy(label).unsqueeze(0).long()
                )  # Convertit en (1, H, W)
            else:
                label = torch.from_numpy(label).long()  # Si déjà (1, H, W)

        return self.normalize(img), label

    def __len__(self):
        """Returns the length of the dataset."""
        if self.mode.lower() == "train":
            return len(self.train_data)
        elif self.mode.lower() == "val":
            return len(self.val_data)
        elif self.mode.lower() == "test":
            return len(self.test_data)
        elif self.mode.lower() == "vis":
            return len(self.test_data)
        else:
            raise RuntimeError(
                "Unexpected dataset mode. " "Supported modes are: train, val and test"
            )
